# AI Safety Farce

Major AI (LLM) companies like Anthropic and OpenAI pride themselves by investing into AI safety research (which they mainly define as AI alignment aka prevent the agent from going rogue) - but they keep a blind eye on safely deploying AI for society as whole, by not investing heavily into private LLM inference (via on-device inference or homomorphic encryption), which would make LLMs private and secure (in the interest of the user) and would prevent providers from collecting user data (which is not the interest of these companies).

Instead they are building the most sophisticated mass digital surveillance machine the world has ever seen. They are normalizing a future where we share every intimate detail of our lives with their free or cheap chatbot service and we let these chatbots not only monitor everything about us but even worse we let us manipulate by them (by consuming their responses / content).

If these companies were truly interested in developing safe AI for humanity they would not build this LLM powered mass manipulation system but instead invest heavily into safe AI deployment via decentral and private LLM inference using  homomorphic encryption and on-device inference. Only this way can AI be used without the risk for mass surveillance and manipulation.

It boils down to:
AI alignment without decentralization still concentrates power.
Concentrated AI power is a societal risk.
Therefore AI deployment architecture is as important as AI alignment.

#AI #privacy
